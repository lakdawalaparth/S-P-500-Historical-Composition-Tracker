{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yearly S&P 500 Holdings and saving output into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-03 17:41:47,693 - INFO - Starting S&P 500 holdings generation\n",
      "2025-01-03 17:41:48,171 - INFO - Retrieved historical data\n",
      "2025-01-03 17:41:48,291 - INFO - Created holdings for 18 years\n",
      "2025-01-03 17:41:48,298 - INFO - Saved holdings for 2008 to sp500_holdings/sp500_holdings_2008.csv\n",
      "2025-01-03 17:41:48,302 - INFO - Saved holdings for 2009 to sp500_holdings/sp500_holdings_2009.csv\n",
      "2025-01-03 17:41:48,306 - INFO - Saved holdings for 2010 to sp500_holdings/sp500_holdings_2010.csv\n",
      "2025-01-03 17:41:48,311 - INFO - Saved holdings for 2011 to sp500_holdings/sp500_holdings_2011.csv\n",
      "2025-01-03 17:41:48,314 - INFO - Saved holdings for 2012 to sp500_holdings/sp500_holdings_2012.csv\n",
      "2025-01-03 17:41:48,317 - INFO - Saved holdings for 2013 to sp500_holdings/sp500_holdings_2013.csv\n",
      "2025-01-03 17:41:48,319 - INFO - Saved holdings for 2014 to sp500_holdings/sp500_holdings_2014.csv\n",
      "2025-01-03 17:41:48,322 - INFO - Saved holdings for 2015 to sp500_holdings/sp500_holdings_2015.csv\n",
      "2025-01-03 17:41:48,325 - INFO - Saved holdings for 2016 to sp500_holdings/sp500_holdings_2016.csv\n",
      "2025-01-03 17:41:48,329 - INFO - Saved holdings for 2017 to sp500_holdings/sp500_holdings_2017.csv\n",
      "2025-01-03 17:41:48,331 - INFO - Saved holdings for 2018 to sp500_holdings/sp500_holdings_2018.csv\n",
      "2025-01-03 17:41:48,334 - INFO - Saved holdings for 2019 to sp500_holdings/sp500_holdings_2019.csv\n",
      "2025-01-03 17:41:48,337 - INFO - Saved holdings for 2020 to sp500_holdings/sp500_holdings_2020.csv\n",
      "2025-01-03 17:41:48,339 - INFO - Saved holdings for 2021 to sp500_holdings/sp500_holdings_2021.csv\n",
      "2025-01-03 17:41:48,341 - INFO - Saved holdings for 2022 to sp500_holdings/sp500_holdings_2022.csv\n",
      "2025-01-03 17:41:48,344 - INFO - Saved holdings for 2023 to sp500_holdings/sp500_holdings_2023.csv\n",
      "2025-01-03 17:41:48,347 - INFO - Saved holdings for 2024 to sp500_holdings/sp500_holdings_2024.csv\n",
      "2025-01-03 17:41:48,349 - INFO - Saved holdings for 2025 to sp500_holdings/sp500_holdings_2025.csv\n",
      "2025-01-03 17:41:48,369 - INFO - Saved combined holdings file\n",
      "2025-01-03 17:41:48,370 - INFO - Processing completed successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from typing import Dict, List\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Configure basic logging for the script.\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "\n",
    "def parse_dates(date_str):\n",
    "    \"\"\"\n",
    "    Parse dates in multiple formats.\n",
    "    \n",
    "    Args:\n",
    "        date_str: Date string to parse\n",
    "    Returns:\n",
    "        Parsed datetime object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try different date formats\n",
    "        for fmt in [\n",
    "            \"%B %d, %Y\",    # September 23, 2019\n",
    "            \"%Y-%m-%d\",     # 1957-03-04\n",
    "            \"%m/%d/%Y\"      # 03/04/1957\n",
    "        ]:\n",
    "            try:\n",
    "                return pd.to_datetime(date_str, format=fmt)\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        # If none of the specific formats work, try the default parser\n",
    "        return pd.to_datetime(date_str)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Could not parse date: {date_str}. Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_sp500_history() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch and process S&P 500 historical data from Wikipedia.\n",
    "    Returns processed historical data DataFrame.\n",
    "    \"\"\"\n",
    "    url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "    data = pd.read_html(url)\n",
    "    \n",
    "    # Process current companies\n",
    "    current = data[0].iloc[:, [0, 1, 5, 6]].copy()\n",
    "    current.columns = ['ticker', 'name', 'date', 'cik']\n",
    "    current['cik'] = current['cik'].astype(str).str.zfill(10)\n",
    "    current['date'] = current['date'].apply(parse_dates)\n",
    "    \n",
    "    # Process historical changes\n",
    "    adjustments = data[1].copy()\n",
    "    adjustments.columns = ['date', 'ticker_added', 'name_added', \n",
    "                         'ticker_removed', 'name_removed', 'reason']\n",
    "    adjustments['date'] = adjustments['date'].apply(parse_dates)\n",
    "    \n",
    "    # Process additions and removals\n",
    "    additions = (adjustments[~adjustments['ticker_added'].isnull()]\n",
    "                [['date', 'ticker_added', 'name_added']]\n",
    "                .rename(columns={'ticker_added': 'ticker', \n",
    "                               'name_added': 'name'}))\n",
    "    additions['action'] = 'added'\n",
    "    \n",
    "    removals = (adjustments[~adjustments['ticker_removed'].isnull()]\n",
    "                [['date', 'ticker_removed', 'name_removed']]\n",
    "                .rename(columns={'ticker_removed': 'ticker', \n",
    "                               'name_removed': 'name'}))\n",
    "    removals['action'] = 'removed'\n",
    "    \n",
    "    # Combine historical changes\n",
    "    historical = pd.concat([additions, removals])\n",
    "    \n",
    "    # Add missing companies\n",
    "    missing = current[~current['ticker'].isin(historical['ticker'])].copy()\n",
    "    missing['action'] = 'added'\n",
    "    missing = missing[['date', 'ticker', 'name', 'action', 'cik']]\n",
    "    \n",
    "    # Create complete history\n",
    "    complete_history = pd.concat([historical, missing])\n",
    "    complete_history = complete_history.sort_values(['date', 'ticker'])\n",
    "    \n",
    "    # Remove any rows where date parsing failed\n",
    "    complete_history = complete_history.dropna(subset=['date'])\n",
    "    \n",
    "    return complete_history, current\n",
    "\n",
    "def create_yearly_holdings(history: pd.DataFrame, current: pd.DataFrame) -> Dict[int, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Create yearly snapshots of S&P 500 holdings from 2008 to present.\n",
    "    \n",
    "    Args:\n",
    "        history: DataFrame with historical changes\n",
    "        current: DataFrame with current companies\n",
    "    Returns:\n",
    "        Dictionary with year as key and holdings DataFrame as value\n",
    "    \"\"\"\n",
    "    # Initialize with current holdings\n",
    "    holdings: Dict[int, set] = {}\n",
    "    current_year = dt.datetime.now().year\n",
    "    \n",
    "    # Create set of current tickers\n",
    "    current_tickers = set(current['ticker'])\n",
    "    \n",
    "    # Initialize all years from 2008 to present with current holdings\n",
    "    for year in range(2008, current_year + 1):\n",
    "        holdings[year] = current_tickers.copy()\n",
    "    \n",
    "    # Process historical changes backwards\n",
    "    for _, row in history.sort_values('date', ascending=False).iterrows():\n",
    "        year = row['date'].year\n",
    "        if year >= 2008:\n",
    "            if row['action'] == 'added':\n",
    "                # Before addition, company wasn't in index\n",
    "                for y in range(2008, row['date'].year + 1):\n",
    "                    holdings[y].discard(row['ticker'])\n",
    "            else:  # removal\n",
    "                # Before removal, company was in index\n",
    "                for y in range(2008, row['date'].year + 1):\n",
    "                    holdings[y].add(row['ticker'])\n",
    "\n",
    "    # Convert sets to DataFrames with company info\n",
    "    yearly_holdings = {}\n",
    "    company_info = pd.concat([\n",
    "        current[['ticker', 'name', 'cik']],\n",
    "        history[['ticker', 'name']].drop_duplicates()\n",
    "    ]).drop_duplicates('ticker')\n",
    "    \n",
    "    for year in holdings:\n",
    "        year_df = (\n",
    "            pd.DataFrame({'ticker': list(holdings[year])})\\\n",
    "            .merge(company_info, on='ticker', how='left')\n",
    "        )\n",
    "        year_df['year'] = year\n",
    "        year_df = year_df[['year', 'ticker', 'name', 'cik']]\n",
    "        yearly_holdings[year] = year_df\n",
    "    \n",
    "    return yearly_holdings\n",
    "\n",
    "def save_yearly_holdings(holdings: Dict[int, pd.DataFrame], output_dir: str = 'sp500_holdings'):\n",
    "    \"\"\"\n",
    "    Save yearly holdings to CSV files.\n",
    "    \n",
    "    Args:\n",
    "        holdings: Dictionary with year as key and holdings DataFrame as value\n",
    "        output_dir: Directory to save the files\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save individual year files\n",
    "    for year, df in holdings.items():\n",
    "        filename = f\"{output_dir}/sp500_holdings_{year}.csv\"\n",
    "        df.to_csv(filename, index=False)\n",
    "        logging.info(f\"Saved holdings for {year} to {filename}\")\n",
    "    \n",
    "    # Create combined file\n",
    "    combined = pd.concat(holdings.values())\n",
    "    combined.to_csv(f\"{output_dir}/sp500_holdings_all_years.csv\", index=False)\n",
    "    logging.info(\"Saved combined holdings file\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the S&P 500 holdings generation.\"\"\"\n",
    "    setup_logging()\n",
    "    logging.info(\"Starting S&P 500 holdings generation\")\n",
    "    \n",
    "    try:\n",
    "        # Get historical data\n",
    "        history, current = get_sp500_history()\n",
    "        logging.info(\"Retrieved historical data\")\n",
    "        \n",
    "        # Create yearly holdings\n",
    "        yearly_holdings = create_yearly_holdings(history, current)\n",
    "        logging.info(f\"Created holdings for {len(yearly_holdings)} years\")\n",
    "        \n",
    "        # Save results\n",
    "        save_yearly_holdings(yearly_holdings)\n",
    "        logging.info(\"Processing completed successfully\")\n",
    "        \n",
    "        return yearly_holdings\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main processing: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    holdings = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yearly S&P 500 Holdings and Storing into database using sqlite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-04 10:19:25,425 - INFO - Starting S&P 500 holdings generation and database creation\n",
      "2025-01-04 10:19:26,099 - INFO - Retrieved historical data\n",
      "2025-01-04 10:19:26,207 - INFO - Created holdings for 18 years\n",
      "2025-01-04 10:19:26,333 - INFO - Saved 794 companies to database\n",
      "2025-01-04 10:19:26,365 - INFO - Saved holdings for 18 years to database\n",
      "2025-01-04 10:19:26,375 - INFO - Saved 964 historical changes to database\n",
      "2025-01-04 10:19:26,377 - INFO - Saved data to database successfully\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "def setup_database(db_path: str = 'sp500_holdings.db') -> sqlite3.Connection:\n",
    "    \"\"\"\n",
    "    Create SQLite database and required tables.\n",
    "    \n",
    "    Args:\n",
    "        db_path: Path to the SQLite database file\n",
    "    Returns:\n",
    "        Database connection object\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    # Create tables\n",
    "    with conn:\n",
    "        # Companies table to store unique company information\n",
    "        conn.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS companies (\n",
    "            ticker TEXT PRIMARY KEY,\n",
    "            name TEXT NOT NULL,\n",
    "            cik TEXT\n",
    "        )\n",
    "        ''')\n",
    "        \n",
    "        # Holdings table to store yearly holdings with foreign key to companies\n",
    "        conn.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS holdings (\n",
    "            year INTEGER,\n",
    "            ticker TEXT,\n",
    "            PRIMARY KEY (year, ticker),\n",
    "            FOREIGN KEY (ticker) REFERENCES companies(ticker)\n",
    "        )\n",
    "        ''')\n",
    "        \n",
    "        # Historical changes table to track additions and removals\n",
    "        conn.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS historical_changes (\n",
    "            date DATE NOT NULL,\n",
    "            ticker TEXT NOT NULL,\n",
    "            name TEXT NOT NULL,\n",
    "            action TEXT CHECK(action IN ('added', 'removed')),\n",
    "            PRIMARY KEY (date, ticker, action)\n",
    "        )\n",
    "        ''')\n",
    "    \n",
    "    return conn\n",
    "\n",
    "def save_to_database(\n",
    "    yearly_holdings: Dict[int, pd.DataFrame],\n",
    "    history: pd.DataFrame,\n",
    "    db_path: str = 'sp500_holdings.db'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save S&P 500 holdings data to SQLite database.\n",
    "    \n",
    "    Args:\n",
    "        yearly_holdings: Dictionary with year as key and holdings DataFrame as value\n",
    "        history: DataFrame with historical changes\n",
    "        db_path: Path to the SQLite database file\n",
    "    \"\"\"\n",
    "    conn = setup_database(db_path)\n",
    "    \n",
    "    try:\n",
    "        with conn:\n",
    "            # Extract unique company information\n",
    "            companies = pd.concat([\n",
    "                df[['ticker', 'name', 'cik']] for df in yearly_holdings.values()\n",
    "            ]).drop_duplicates('ticker')\n",
    "            \n",
    "            # Save companies\n",
    "            companies.to_sql('companies', conn, if_exists='replace', index=False)\n",
    "            logging.info(f\"Saved {len(companies)} companies to database\")\n",
    "            \n",
    "            # Save holdings for each year\n",
    "            holdings_data = []\n",
    "            for year, df in yearly_holdings.items():\n",
    "                for ticker in df['ticker']:\n",
    "                    holdings_data.append({'year': year, 'ticker': ticker})\n",
    "            \n",
    "            holdings_df = pd.DataFrame(holdings_data)\n",
    "            holdings_df.to_sql('holdings', conn, if_exists='replace', index=False)\n",
    "            logging.info(f\"Saved holdings for {len(yearly_holdings)} years to database\")\n",
    "            \n",
    "            # Save historical changes\n",
    "            history_df = history[['date', 'ticker', 'name', 'action']].copy()\n",
    "            history_df.to_sql('historical_changes', conn, if_exists='replace', index=False)\n",
    "            logging.info(f\"Saved {len(history_df)} historical changes to database\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving to database: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def query_holdings_for_year(year: int, db_path: str = 'sp500_holdings.db') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Query holdings for a specific year.\n",
    "    \n",
    "    Args:\n",
    "        year: Year to query\n",
    "        db_path: Path to the SQLite database file\n",
    "    Returns:\n",
    "        DataFrame with holdings for the specified year\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    try:\n",
    "        query = '''\n",
    "        SELECT h.year, h.ticker, c.name, c.cik\n",
    "        FROM holdings h\n",
    "        JOIN companies c ON h.ticker = c.ticker\n",
    "        WHERE h.year = ?\n",
    "        ORDER BY h.ticker\n",
    "        '''\n",
    "        \n",
    "        return pd.read_sql_query(query, conn, params=[year])\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def query_company_history(ticker: str, db_path: str = 'sp500_holdings.db') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Query historical changes for a specific company.\n",
    "    \n",
    "    Args:\n",
    "        ticker: Company ticker symbol\n",
    "        db_path: Path to the SQLite database file\n",
    "    Returns:\n",
    "        DataFrame with historical changes for the company\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    try:\n",
    "        query = '''\n",
    "        SELECT date, ticker, name, action\n",
    "        FROM historical_changes\n",
    "        WHERE ticker = ?\n",
    "        ORDER BY date\n",
    "        '''\n",
    "        \n",
    "        return pd.read_sql_query(query, conn, params=[ticker])\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Modify the main function to include database operations\n",
    "def main():\n",
    "    \"\"\"Main function to generate S&P 500 holdings and save to database.\"\"\"\n",
    "    setup_logging()\n",
    "    logging.info(\"Starting S&P 500 holdings generation and database creation\")\n",
    "    \n",
    "    try:\n",
    "        # Get historical data\n",
    "        history, current = get_sp500_history()\n",
    "        logging.info(\"Retrieved historical data\")\n",
    "        \n",
    "        # Create yearly holdings\n",
    "        yearly_holdings = create_yearly_holdings(history, current)\n",
    "        logging.info(f\"Created holdings for {len(yearly_holdings)} years\")\n",
    "        \n",
    "        # Save to database\n",
    "        save_to_database(yearly_holdings, history)\n",
    "        logging.info(\"Saved data to database successfully\")\n",
    "        \n",
    "        return yearly_holdings\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main processing: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    holdings = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Yearly Output from databse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current Companies in S&P 500:\n",
      "    ticker                        name         cik\n",
      "0      AXP            American Express  0000004962\n",
      "1      HCA              HCA Healthcare  0000860730\n",
      "2      CRL  Charles River Laboratories  0001100682\n",
      "3      FCX            Freeport-McMoRan  0000831259\n",
      "4      AIZ                    Assurant  0001267238\n",
      "..     ...                         ...         ...\n",
      "497    CPB       Campbell Soup Company  0000016732\n",
      "498    ESS        Essex Property Trust  0000920522\n",
      "499    SRE                      Sempra  0001032208\n",
      "500    DAL             Delta Air Lines  0000027904\n",
      "501   APTV                       Aptiv  0001521332\n",
      "\n",
      "[502 rows x 3 columns]\n",
      "\n",
      "Changes made in 2023:\n",
      "                   date ticker                          name   action\n",
      "0   2023-01-04 00:00:00   GEHC                 GE HealthCare    added\n",
      "1   2023-01-05 00:00:00    VNO          Vornado Realty Trust  removed\n",
      "2   2023-03-15 00:00:00     BG                  Bunge Global    added\n",
      "3   2023-03-15 00:00:00   PODD                       Insulet    added\n",
      "4   2023-03-15 00:00:00   SBNY                Signature Bank  removed\n",
      "5   2023-03-15 00:00:00   SIVB           SVB Financial Group  removed\n",
      "6   2023-03-20 00:00:00   FICO                    Fair Isaac    added\n",
      "7   2023-03-20 00:00:00   LUMN            Lumen Technologies  removed\n",
      "8   2023-05-04 00:00:00   AXON               Axon Enterprise    added\n",
      "9   2023-05-04 00:00:00    FRC           First Republic Bank  removed\n",
      "10  2023-06-20 00:00:00   DISH                  Dish Network  removed\n",
      "11  2023-06-20 00:00:00   PANW            Palo Alto Networks    added\n",
      "12  2023-08-25 00:00:00    AAP            Advance Auto Parts  removed\n",
      "13  2023-08-25 00:00:00   KVUE                        Kenvue    added\n",
      "14  2023-09-18 00:00:00   ABNB                        Airbnb    added\n",
      "15  2023-09-18 00:00:00     BX                    Blackstone    added\n",
      "16  2023-09-18 00:00:00    LNC  Lincoln National Corporation  removed\n",
      "17  2023-09-18 00:00:00    NWL                 Newell Brands  removed\n",
      "18  2023-10-02 00:00:00   VLTO                       Veralto    added\n",
      "19  2023-10-03 00:00:00    DXC                DXC Technology  removed\n",
      "20  2023-10-18 00:00:00   ATVI           Activision Blizzard  removed\n",
      "21  2023-10-18 00:00:00   HUBB                       Hubbell    added\n",
      "22  2023-10-18 00:00:00   LULU           Lululemon Athletica    added\n",
      "23  2023-10-18 00:00:00    OGN                 Organon & Co.  removed\n",
      "24  2023-12-18 00:00:00    ALK              Alaska Air Group  removed\n",
      "25  2023-12-18 00:00:00   BLDR          Builders FirstSource    added\n",
      "26  2023-12-18 00:00:00    JBL                         Jabil    added\n",
      "27  2023-12-18 00:00:00   SEDG                     SolarEdge  removed\n",
      "28  2023-12-18 00:00:00    SEE                    Sealed Air  removed\n",
      "29  2023-12-18 00:00:00   UBER                          Uber    added\n",
      "\n",
      "First 5 current companies:\n",
      "  ticker                        name         cik\n",
      "0    AXP            American Express  0000004962\n",
      "1    HCA              HCA Healthcare  0000860730\n",
      "2    CRL  Charles River Laboratories  0001100682\n",
      "3    FCX            Freeport-McMoRan  0000831259\n",
      "4    AIZ                    Assurant  0001267238\n",
      "\n",
      "DataFrame shapes:\n",
      "Current companies: (502, 3)\n",
      "2023 changes: (30, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Connect and query directly to DataFrame\n",
    "conn = sqlite3.connect('sp500_holdings.db')\n",
    "\n",
    "# Get all companies currently in the index\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT c.*\n",
    "FROM companies c\n",
    "JOIN holdings h ON c.ticker = h.ticker\n",
    "WHERE h.year = 2024\n",
    "\"\"\"\n",
    "current_companies = pd.read_sql_query(query, conn)\n",
    "print(\"\\nCurrent Companies in S&P 500:\")\n",
    "print(current_companies)  # Display the current companies DataFrame\n",
    "\n",
    "# Get historical changes for a specific year\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM historical_changes\n",
    "WHERE strftime('%Y', date) = '2023'\n",
    "ORDER BY date\n",
    "\"\"\"\n",
    "changes_2023 = pd.read_sql_query(query, conn)\n",
    "print(\"\\nChanges made in 2023:\")\n",
    "print(changes_2023)  # Display the changes DataFrame\n",
    "\n",
    "# You can also see the first few rows using head()\n",
    "print(\"\\nFirst 5 current companies:\")\n",
    "print(current_companies.head())\n",
    "\n",
    "# And check the shape (number of rows and columns) of your DataFrames\n",
    "print(\"\\nDataFrame shapes:\")\n",
    "print(f\"Current companies: {current_companies.shape}\")\n",
    "print(f\"2023 changes: {changes_2023.shape}\")\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
